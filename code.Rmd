---
title: An R Markdown document converted from "code.ipynb"
output: html_document
---

Le code est fourni en format notebook R (code.rmd) et Jupyter (code.ipynb). Le contenu des deux fichiers est identique.

```{r}
# Pour ne pas afficher les warnings
options(warn = -1)

# Pour contrôler les dimensions des graphes
figsize <- function(width, heigth) {
     options(repr.plot.width = width, repr.plot.height = heigth)
}
```

```{r}
# Ce code permet de charger les données à analyser, le données sont disponible via le lien:
# https://www.insee.fr/fr/statistiques/serie/010537233
# La librairie ts permet de declarer la base de données comme une serie temporelle

data <- read.csv("data.csv")
serie <- ts(
    data$Valeur,
    start = c(1990, 1),
    frequency = 12
    )

# Visualisation de la série
serie
```

# Question 1

```{r}
# Représentation de graphique de la serie: Evolution de l'indice de production alimentaire
figsize(10, 7)
plot(serie, col = 4, lwd = 3)
```

On entrainera le modèle sur les données jusqu'en 2018, puis on cherchera à prédire les données de 2019. Cela nous permet d'éviter la période de la crise du Covid-19.

```{r}
# Sélection et représentation des données sur la période d'analyse #

serie <- window(
    serie,
    end = c(2018, 12)
)
plot(
    serie,
    ylab = "IPI Alimentaire",
    main = "Evolution de l'indice de la production alimentaire",
    col = 4, lwd = 3
)
```

Comme dans toute analyse de serie temporelle il est important de voir la decomposition de notre serie, notamment
  - la composante saisonnière;
  - la composante tendancielle.
  
Ici nous n'avons pas de problème de saisonnalité car série est corrigée des variations saisonnières.

```{r}
# Ce code permet de visualiser la composition de notre série.
plot(decompose(serie), col = 1, lwd = 3)
```

Graphiquement notre serie semble non stationnaire et possède une tendance à la hausse.

On vérifie que la série n'est pas stationnaire avec les tests usuels:
- ADF;
- PP;
- KPSS

### Dickey-Fuller augmenté

```{r}
# Le test de Dickey-Fuller augmenté joint avec le test de Ljung Box (pour l'autocorrelation des résidus)
dickey_fuller <- function(x, ks = 0:24, type = "ct") {
    adf_p_values <- c()
    lb_p_values <- c()
    for (k in ks) {
        adf <- fUnitRoots::adfTest(x, lags = k, type = type)
        adf_p_val <- adf@test$p.value
        lb_p_val <- Box.test(adf@test$lm$residuals, 24)$p.value
        adf_p_values <- c(adf_p_values, adf_p_val)
        lb_p_values <- c(lb_p_values, lb_p_val)
    }

    df <- data.frame(
        Lag = ks,
        ADF_Test = adf_p_values,
        Ljung_Box_Residuals = lb_p_values
    )

    return(df)
}

dickey_fuller(serie, type = "ct", ks = 0:5)
```

Le test ADF rejette la racine unitaire pour les lags<5, mais il n'est pas forcément vrai
car on n'a pas vérifier la validité des résidus qui vérifié par le test de Ljung Box.
Finalement à partir des lags 5 ADF ne rejette pas l'hypothèse de racine unité  et cela est valide avec le test de Ljung Box
(On a du ajouter $5$ retards pour obtenir des résidus non autocorrélés.)

### Philipps-Perron

```{r}
tseries::pp.test(serie)
```

Phillips-Perron rejette l'hypothèse de racine unité.

### KPSS

```{r}
tseries::kpss.test(serie)
```

KPSS rejette la stationnarité.

### Conclusion sur la stationnarité de la série brute.
Deux de nos test conclurent sur la non stationnarité de la serie étudié. et donc nous considérons que notre serie
est non statitionaire par la suite, ce qui confirme l'observation visuelle.

# Question 2 - Stationnarisation

```{r}
#Code pour différencier la série en vue de la rendre stationnaire.
diff1 <- diff(serie, 1) # différence première de la série
# Visualisation des données différenciées
diff1
```

```{r}
# Représentation jointe de la serie différenciée et la serie brute
par(mfrow = c(1, 2))
figsize(20, 7)
plot(
    serie,
    ylab = "Série ",
    main = "Evolution de l'indice de la production alimentaire",
    col = 1, lwd = 3
    )
plot(
    diff1,
    ylab = "Série différenciée",
    main = "Evolution de l'indice de la production alimentaire différencié",
    col = 1, lwd = 3
    )
```

Visuellement, la série différenciée semble stationnaire et de tendance nulle. Nous allons le vérifier avec les tests de stationnarité utlisés précédemment

### Test de Dickey-Fuller

```{r}
dickey_fuller(diff1, type = "nc", ks = 0:5)
```

Le test de Dickey-Fuller augmenté rejette donc l'hypothèse de racine unité.

### Test de Phillips-Perron

```{r}
tseries::pp.test(diff1)
```

Idem pour le test de Phillips-Perron

### Test KPSS

```{r}
tseries::kpss.test(diff1)
```

Le test KPSS ne rejette pas l'hypothèse de stationnarité.

### Conclusion sur la stationnarité de la série différenciée.
Les trois tests sont unanimes et indiquent que la série différenciée est stationnaire.

# Questions 4 & 5 - Modélisation ARMA

```{r}
# Ces librairies sont nécessaires pour les graphiques suivants
library(ggplot2)
library(patchwork)
# Et pour la modélisation ARMA
library(forecast)

# Code pour visualiser l'ACF et le PACF
figsize(20, 5)
(ggAcf(diff1, lag.max = 12,lwd=2, col=1) + labs(title = "ACF - Série différenciée")
     + ggPacf(diff1, lag.max = 12,lwd=2, col=4) + labs(title = "PACF - Série différenciée"));
```

Les autocorrélations sont significatives jusqu'à l'ordre 1, donc $q_{max}=1$.
Les autocorrélations partielles sont significatives jusqu'à l'ordre 7, donc $p_{max}=7$

### Choix du modèle :

```{r}
# Ce code permet de créer un fonction pour calculer les valeurs de l'AIC et du BIC selon les ordres p, q et d du modèle ARIMA
evaluate_models_ic <- function(pmax, d, qmax, x = serie) {
  ps <- c()
  ds <- c()
  qs <- c()
  aics <- c()
  bics <- c()
  for (q in 0:qmax) {
    for (p in 0:pmax) {
      ps <- c(ps, p)
      ds <- c(ds, d)
      qs <- c(qs, q)
      model <- forecast::Arima(x, order = c(p, d, q))
      aics <- c(aics, AIC(model))
      bics <- c(bics, BIC(model))
    }
  }

  df <- data.frame(
        p = ps,
        d = ds,
        q = qs,
        AIC = aics,
        BIC = bics
    )

    return(df)
}

# On visualise les resultats en fonction de p, q et d
tab <- evaluate_models_ic(pmax = 7, d = 1, qmax = 1)
tab
```

L'AIC est minimisé en par l'ARIMA(1,1,1) et le BIC par l'ARIMA(0,1,1)

```{r}
# Test de validation des résidus
figsize(10, 7)
res01 <- astsa::sarima(serie, 0, 1, 1)
res11 <- astsa::sarima(serie, 1, 1, 1)
```

Pour les modèles ARIMA(0,1,1) et ARIMA(1,1,1), les tests de Ljung-Box et les auto-corrélogrammes semblent indiquer que les résidus sont des bruits blancs. <br>Pour trancher entre ces deux modèles, regardons si l'ARIMA(1,1,1) peut être simplifié :

```{r}
# Estimation du modèle (MA(1))
res01$ttable
```

```{r}
# Estimation du modèle (ARMA(1,1))
res11$ttable
```

Au seuil de 10%, on l'ARIMA(1,1,1) ne peut pas être simplifié. On conserve donc ce modèle. On a alors :

$$
    X_t - 0,16X_{t-1} = 0,03 + \epsilon_t - 0,72\epsilon_{t-1}
$$

Ainsi, pour la série d'origine (notée $Y_t$), comme $X_t = Y_t - Y_{t-1}$, on a $Y_t = Y_{t-1}+X_t$ et donc

\begin{align}
    Y_t & = Y_{t-1} + 0,16X_{t-1} + 0,03 + \epsilon_t - 0,72\epsilon_{t-1}  \\
    Y_t & = Y_{t-1} + 0,16(Y_{t-1}-Y_{t-2}) + 0,03 + \epsilon_t - 0,72\epsilon_{t-1}  \\
    Y_t & = 1,16Y_{t-1} - 0,16Y_{t-2} + 0,03 + \epsilon_t - 0,72\epsilon_{t-1}
\end{align}

# Partie 3 - Prévisions

La region de confiance de la prévision de 


$ \begin{pmatrix} X_{T+1} \\ X_{T+2} \end{pmatrix} $ est donnée par:

\begin{equation}
\boxed{
    \begin{pmatrix}
        \hat X_{T+1|T} \\
        \hat X_{T+2|T}
    \end{pmatrix}
    +
    \left\{ e \in \mathbb{R}^2\ |\ e'\Omega^{-1}e \leq q_{1-\alpha} \right\}
}
\end{equation}

(Les détails du calcul figurent dans le rapport)

Pour estimer cette région de confiance, il faut estimer $\Omega$. Comme $\phi$ et $\psi$ sont déjà estimés, il ne reste qu'à estimer $\sigma$, à partir des résidus observés. La région obtenue ne sera alors valable qu'asymptotiquement.

### Estimation de $\Omega$

```{r}
# Récupération de la variance des résidus
sigma2 <- var(res11$fit$residuals)
sigma2
```

```{r}
# Récupération des coefficients
phi <- res11$fit$coef["ar1"]
psi <- res11$fit$coef["ma1"]

# Calcul de la matrice de variance-covariance.
omega <- sigma2 * matrix(
    c(
        1, phi + psi,
        phi + psi, 1 + (phi + psi)**2
    ),
    nrow = 2,
    ncol = 2
)
omega
```

```{r}
# Inverse de la matrice de variance-covariance
omega_inv <- matlib::inv(omega)
omega_inv
```

```{r}
q95 <- qchisq(.95, df = 2)
q95
```

### Prédiction des valeurs futures

```{r}
# Récupération des valeurs prédites
predictions <- astsa::sarima.for(
    diff1, n.ahead = 2,
    p = 1, d = 0, q = 1,
    plot = TRUE
    )$pred
predictions
```

### Région de confiance

```{r}
region <- ellipse::ellipse(omega, centre = predictions, level = .95)

figsize(7, 5)
par(mar = c(4, 4, 3, 5))

plot(region, type = "l", axes = F, xlab = "", ylab = "")
par(new = T)
plot(predictions[1], predictions[2], axes = F, xlab = "", ylab = "", col = "blue", pch = 16)
par(new = T)
plot(0.25, -0.49, axes = F, xlab = "", ylab = "", col = "black", pch = 3)

axis(1, ylim = c(.15, .35), at = seq(.15, .35, by = .05))
mtext("Prévision pour janvier 2019", side = 1, line = 2.5)
axis(2, ylim = c(-.7, -.3), at = seq(-.7, -.3, by = .1))
mtext("Prévision pour février 2019", side = 2, line = 2.5)
```

